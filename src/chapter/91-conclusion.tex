% !TEX root = ../report.tex

\chapter{Conclusion}

\minitoc

This chapter will describe the final version of our product and what we have found during this project. Also we will discuss the further development of the system and what improvement can be made.

\clearpage

\section{Final Product}
STATUS - beskriv status for ditt arbeid
TODO
We spent a lot of time on the preliminary study and doing research to build the ground for our work. This was to decide which direction to take amongst the vast amount possibilities. We see this as well spent time. Similar solutions~\ref{sec:similarsys} was of great help, and let us understand how different approaches had been done by previous systems and what worked from them.

\subsection{The harvestere}
TODO Something about amanda or whatevah - the scraper

\subsection{The database}
The choice of database was a good match to the data harvested from Twitter. MongoDB is a easily scalable database, with easy replication possibilities. The data from Twitter consists of different kinds of field, and would therefore have been more troublesome to work with in a SQL database. MongoDB, as other document oriented NoSQL databases, stores the data as JavaScript Object Notation, which is the same notation as fetched from Twitter ? TODO, chech this fact lol. And there would therefore not be needed any parsing from the Twitter harvesting to the database.

\subsection{The predictor}
The prediction part was never implemented TODO: refer to something about why. But research around good prediction candidates has been made, and is and interesting topic for future work.


\section{Findings}
It was interesting to see from the research done how many different approaches there are to take when suggesting user ratings, and the importance of diversity when calculating the predictions is key. There is no one model when suggesting movies to users, which will be perfect for all. Even the top two winners of the Netflix-prize competition scored better when their solutions were merged together. Even though the winners managed to produce a 10\% better score than the Netflix's own system, the winners solution was not taken into production. The algorithms used to recommend at the winners level would be too time consuming to use on the complete data, and would not work well with the constant updating values of the actual Netflix users ratings, whereas the Netflix-prize competitors where working on a unknown but stationary set of users and ratings.

TODO - Twitter

\subsection{Goals reached}
TODO - discuss the goals...
The goals reached was

\subsection{Goals unreached}
sammenlignet med hva du opprinnelig ønsket å oppnå.


\section{Related Work}
As we saw from the preliminary study on similar systems~\ref{sec:similarsys}, there are some systems with similar characteristics as the approach taken in this project. They were of great help when exploring how to attack the issues that comes along with recommending movies, gathering information from social media and exploring big amounts of data.

For Twitter Twittomender~\cite{twittomender}, and their approach and research on how to find interesting friends, and thereby finding relevant users to harvest from, was especially helpful.

Papers on different recommendation algorithms and their produced runtime and root-mean-square error, such as \cite{bigchaos-sol,alsMPI,BellKor-CF-TD} was great help when deciding a suitable algorithm to use for rating predictions.

MovieTweetings~\ref{subsec:MovieTweetings} had an interesting take on harvesting and storing of tweets, but their approach on gathering tweets might be too strict, and would seem to reject a lot of potential tweets with the same information gain as the ones they store, but not being on the same form.


\section{Future Work}
\subsection{Prediction algorithms}
There is a lot of potential future work to be done regarding social media and general recommendation. In the domain of movie recommendation the exploration of different recommendation and prediction algorithms is an interesting field. The most central points regarding this work and algorithms are:
\begin{itemize}
    \item Runtime
    \item Score (RMSE)
    \item Runtime versus score (RMSE)
    \item Parallelization
    \item Scalability
\end{itemize}
Runtime and score was just touched during this research, but opened for some interesting future work to be done, with well established testing potential through RMSE when comparing different approaches. This has opened a world of testing potential, where the main goal would be to find the prefect algorithm/algorithms to predict ratings, which was thoroughly explored in the Netflix-prize competition, but then only on a subset of data, with few restrictions. When adding restrictions such as runtime other solutions will emerge, these solutions could prove to be interesting to explore in depth.

When focusing in on the prediction approach taken in this research, different versions should be explored. Variants of k-NN can be used to select neighbors. Other interesting subjects for future work includes:
\begin{itemize}
    \item How to handle outliers in a good way regarding the data
    \item Best approach to value neighbors
    \item Exploring different heuristics for good $k$ values.
\end{itemize}

\subsection{Data modeling}
TODO
Outliers  normalizing dataset building


\subsection{Learners}
Comparing the actual ratings users are giving a movie with the predicted ratings, and having the system learn from these values. This would allow the system to not only depend on the harvested ratings and Netflix ratings, but get an extra source of prediction.

Future work in this field would include:
\begin{itemize}
    \item Exploring how to weigh a correct or faulty prediction
    \item If a long term learning system will be of any help
    \item Cost of having the system learn versus the score gained
    \item Potential scaling issues with learning
\end{itemize}


\subsection{Twitter harvesting}
TODO

