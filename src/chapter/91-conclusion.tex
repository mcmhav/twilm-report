% !TEX root = ../report.tex

\chapter{Conclusion}

\minitoc

This chapter will describe the final version of our product and what we have found during this project. Related work will be discussed and reviewed. And to sum it up, exploring the further development of the system and improvements which can be made to it, and more general future work.

\clearpage

\section{Final Product}
STATUS - beskriv status for ditt arbeid
sammenlignet med hva du opprinnelig ønsket å oppnå.

We spent a lot of time on the preliminary study and doing research to build the ground for our work. This was to decide which direction to take amongst the vast amount possibilities. We see this as well spent time. Similar solutions~\ref{sec:similarsys} was of great help, and let us understand how different approaches had been done by previous systems and what worked from them. This section will look more in depth into the status of the different parts, and conclusions made around them.

\begin{table}[H]
    \centering
    \begin{tabular}{ l | l }
        \textbf{Expectations} & \textbf{Fulfilled} \\ \hline
        Implement harvester & \cmark \\ \hline
        Gathering data & \xmark \\ \hline
        Constructing dataset & \xmark \\ \hline
        Implement prediction system & \xmark \\ \hline
        Comparing RMSE scores & \xmark \\
    \end{tabular}
    \captionof{table}[Goals]{This is a table of the goals. The fulfilled goals are marked with a \cmark, and the unfulfilled are marked with a \xmark}
    \label{tab:reached-goals}
\end{table}

\subsection{The fields}
\subsubsection{Done}
The fields Stream, Scraper and REST were designed and implemented. Scraper could not be tested due to legal considerations.
\subsubsection{Not Done}
Search was not implemented due to the fact that it was deemed to be similar but inferior to Stream.

\subsection{The harvesters}
\subsubsection{Done}
The harvesters NetflixMovieTweetScrape
and TwitterUserFolloweeREST were designed and implemented.
\subsubsection{Not Done}
TwitterUserFollowerREST was not implemented. It was useful in theory but not in practice due to the rate limit.

\subsection{The database}
\subsubsection{Done}
The choice of database was a good match to the data harvested from Twitter. MongoDB is a easily scalable database and fast, with easy replication possibilities. The data from Twitter consists of different kinds of fields, and would therefore have been more troublesome to work with in a SQL database. MongoDB, as other document oriented NoSQL databases, stores the data as JavaScript Object Notation, which is the same notation as fetched from the Twitter APIs. There are modules for data conversion to and from JSON for any thinkable datastructure.

\subsubsection{Not Done}


\subsection{The predictor}
\subsubsection{Done}
It was interesting to see from the research done how many different approaches there are to take when suggesting user ratings, and the importance of diversity when calculating the predictions is key. There is no one model when suggesting movies to users, which will be perfect for all. Even the top two winners of the Netflix-prize competition scored better when their solutions were merged together. Even though the winners managed to produce a 10\% better score than the Netflix's own system, the winners solution was not taken into production. The algorithms used to recommend at the winners level would be too time consuming to use on the complete data, and would not work well with the constant updating values of the actual Netflix users ratings, whereas the Netflix-prize competitors where working on a unknown but stationary set of users and ratings.

\subsubsection{Not Done}
The prediction part was never implemented because of the missing dataset as described in \ref{sec:issues}. But research around good prediction candidates has been made, and is and interesting topic for future work. Especially the use of k-NN for gathering similar datapoints to make rating predictions with.





\section{Related Work}
As we saw from the preliminary study on similar systems~\ref{sec:similarsys}, there are some systems with similar characteristics as the approach taken in this project. They were of great help when exploring how to attack the issues that comes along with recommending movies, gathering information from social media and exploring big amounts of data.

For Twitter Twittomender~\cite{twittomender}, and their approach and research on how to find interesting friends, and thereby finding relevant users to harvest from, was especially helpful.

Papers on different recommendation algorithms and their produced runtime and root-mean-square error, such as \cite{bigchaos-sol,alsMPI,BellKor-CF-TD} was great help when deciding a suitable algorithm to use for rating predictions.

MovieTweetings~\ref{subsec:MovieTweetings} had an interesting take on harvesting and storing of tweets, but their approach on gathering tweets might be too strict, and would seem to reject a lot of potential tweets with the same information gain as the ones they store, but not being on the same form.




\section{Future Work}
\subsection{Twitter harvesting}
TODO:fr



\subsection{Prediction algorithms}
There is a lot of potential future work to be done regarding social media and general recommendation. In the domain of movie recommendation the exploration of different recommendation and prediction algorithms is an interesting field. The most central points regarding this work and algorithms are:

\begin{itemize}
    \item Runtime
    \item Score (RMSE)
    \item Runtime versus score (RMSE)
    \item Parallelization
    \item Scalability
\end{itemize}

Runtime and score was just touched during this research, but opened for some interesting future work to be done, with well established testing potential through RMSE when comparing different approaches. This has opened a world of testing potential, where the main goal would be to find the most suiting algorithm/algorithms to predict ratings, which was thoroughly explored in the Netflix-prize competition, but then only on a subset of data, with few restrictions. When adding restrictions such as runtime, other solutions will emerge, these solutions could prove to be interesting to explore in depth.

When focusing in on the prediction approach taken in this research, different variants of k-NN should be explored. The different variants can be used to select neighbors. Other interesting subjects for future work includes:

\begin{itemize}
    \item How to handle outliers in a good way regarding the data
    \item Best approach to value/weigh neighbors
    \item Exploring different heuristics for good $k$ values.
\end{itemize}



\subsection{Data modeling}
The approach taken in this study for building the dataset with the Netflix-dataset and the data from Twitter can get issues with outliers, and must be research further. If the data is blindly normalized, outliers might shift datapoints away form their actual position, and construct a shifted illusion of their value.


\subsection{Learners}
Comparing the actual ratings users are giving a movie with the predicted ratings, and having the system learn from these values. This would allow the system to not only depend on the harvested ratings and Netflix ratings, but get an extra source of prediction.

Future work in this field would include:
\begin{itemize}
    \item Exploring how to weigh a correct or faulty prediction
    \item If a long term learning system will be of any help
    \item Cost of having the system learn versus the score gained
    \item Potential scaling issues with learning
\end{itemize}


